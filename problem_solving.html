<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>WA - CS & Algorithms</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/browning.jpg" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=https://fonts.googleapis.com/css?family=Inconsolata:400,500,600,700|Raleway:400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">


</head>

<body>

  

  <nav class="navbar navbar-light custom-navbar">
    <div class="container">
      <a class="navbar-brand" href="index.html">Washieu Anan</a>        <span></span>
      </a>
    </div>
  </nav>

  <main id="main">

    <section class="section">
      <div class="container">
        <div class="row mb-4 align-items-center">
          <div class="col-md-6" data-aos="fade-up">
            <h2>CS & Algorithms</h2>
            <p></p>
          </div>
        </div>
      </div>

      <div class="site-section pb-0">
        <div class="container">
          <div class="row align-items-stretch">
            <div class="col-md-8" data-aos="fade-up">
              <img src="assets/img/6dpose.gif" alt="Image" class="img-responsive">
            </div>
            <br>
            &nbsp;
           
            <br>
            <div class="col-md-4 ml-auto" data-aos="fade-up" data-aos-delay="100">
              <div class="sticky-content">
                <br>
                <h3 class="h3">Computer Vision</h3>
                <p class="mb-4"><span class="text-muted"></span></p>

                <div class="mb-5">
                  <p>
                    Computer vision is a field that encompasses many different topics, including object detection, pose estimation, classification, medical imaging, etc. In the FIRST Tech Challenge context, 
                    computer vision is typically assumed to be used for object detection. At the beginning of the autonomous periods, the robot, using cameras, is to identify some game element that dictates the
                    action of the robot. For example, in the ULTIMATE GOAl 2020-2021 FTC Challenge, robots had to identify the number of rings in the randomized starting-stack at the beginning of autonomous, determining where the robot should place the wobble goal game element. 
                   
                    <br>
                    <br>

                    As linked <a href="https://www.youtube.com/watch?v=F2JgAv-Ky7g">here,</a> we can see some of my work involving a robust object detection algorithm to identify the number of rings inside the starting-stack
                    during the autonomous period in the ULTIMATE GOAL 2021-2022 FTC Season. While we did not implement this feature into our autonomous because of our localization drift (primarily because of the localization method), we were also successful in
                    developing an algorithm that would be able to identify the (x,y) position of a ring on the field. This would useful in autonomous to collect rings other than the preloaded rings. 
                    While I have now deleted the video that demonstrates the algorithm working, here is the code: <a href="https://github.com/BrowningUltro-10539/Ultimate-Goal/blob/master/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/Experimental/Vision/RingLocator/RingLocatorPipeline.java">https://tinyurl.com/3yxkufsz</a>

                    
                    <br>
                    <br>

                    For the FREIGHT FRENZY 2021-2022 FTC Season, I decided to tackle an issue that many teams were facing with a unique approach. Prior the match, the organization of freight in the warehouse is randomized. 
                    This poses a problem to autonomous programs that plan on scoring freight from the warehouse as many teams build their programs on pre-inputted encoder or coordinate values. This is not as accurate, however, given that many teams miss at least one-two
                    pieces of freight in the autonomous period. What if there was a way to adapt to the randomization of the freight in the warehouse? A form of pose estimation seemed to be the answer. 

                    <br>
                    <br>

                  
                    Thus, I began to read various research papers that published their work in achieving six-dimensional pose estimation. The issue with pose estimation was the high-resource enviornment it required to compute the position and translational vectors associated with the object. 
                    There were, however, some papers that detailed how pose estimation was possible in a low-resource enviornment, similar to the one we have in the Control Hub. Using their techniques and the custom dataset that I developed for freight localization, I was able to achieve freight localization at 1 FPS. 
                    1 FPS seems low but it worked well enough for our purposes as we would simply have to capture an image when entering the warehouse, allowing us to extract information from the image. 

                    <br>
                    <br>
                    Using the coordinate information I received from the model, I was not able to relocalize the robot but I was also able to understand where pieces of freight were on the field. While never implemented on a competition bot, the robot in testing was able to drive to a piece of freight correctly 99% of the time.
                    
                    <br>
                    <br>
                    Code Based Off: <a href="https://github.com/thodan/t-less_toolkit">https://github.com/thodan/t-less_toolkit</a>
                  </p>

                  <img src="assets/img/freight_pose.png" alt="Image" class="img-responsive">
                  <br>
                  <br>

                  <h3 class="h3">Localization & Movement</h3>
                  <p class="mb-4"><span class="text-muted"></span></p>
                  <p>
                    Localization and movement work can be found in the <a href="problem_solving.html">Engineering Design Process page.</a>
                  </p>
                </div>

                
                
              </div>

              
             
              </div>
            </div>
          </div>
        </div>
    </section>



  <!-- ======= Footer ======= -->
  <footer class="footer" role="contentinfo">
    <div class="container">
      <div class="row">
        <div class="col-sm-6">
          <p class="mb-1">&copy; Copyright Washieu Anan. All Rights Reserved</p>
          <div class="credits">
       
        
            Designed by <a href="https://github.com/washieuanan">Washieu Anan</a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>